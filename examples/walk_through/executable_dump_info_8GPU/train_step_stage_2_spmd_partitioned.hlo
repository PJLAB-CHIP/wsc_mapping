HloModule train_step_pipeshard_parallel_mesh_1.48-layer_1_backward, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias) }, entry_computation_layout={(f32[64]{0},f32[128,64]{1,0},f32[128]{0},f32[64,128]{1,0},f32[8,64]{1,0},f32[8,128]{1,0},f32[128,64]{1,0},f32[8,64]{1,0},f32[64,128]{1,0})->(f32[64]{0}, f32[128,64]{1,0}, f32[128]{0}, f32[64,128]{1,0}, f32[8,64]{1,0})}

%region_1.161.layer_1_backward (Arg_0.162: f32[], Arg_1.163: f32[]) -> f32[] {
  %Arg_0.162 = f32[] parameter(0)
  %Arg_1.163 = f32[] parameter(1)
  ROOT %add.164 = f32[] add(f32[] %Arg_0.162, f32[] %Arg_1.163), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add (x: f32[], y: f32[]) -> f32[] {
  %x = f32[] parameter(0)
  %y = f32[] parameter(1)
  ROOT %add = f32[] add(f32[] %x, f32[] %y)
}

%add.1 (x.1: f32[], y.1: f32[]) -> f32[] {
  %x.1 = f32[] parameter(0)
  %y.1 = f32[] parameter(1)
  ROOT %add.1 = f32[] add(f32[] %x.1, f32[] %y.1)
}

%region_2.169.layer_1_backward (Arg_0.170: f32[], Arg_1.171: f32[]) -> f32[] {
  %Arg_0.170 = f32[] parameter(0)
  %Arg_1.171 = f32[] parameter(1)
  ROOT %add.172 = f32[] add(f32[] %Arg_0.170, f32[] %Arg_1.171), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.2 (x.2: f32[], y.2: f32[]) -> f32[] {
  %x.2 = f32[] parameter(0)
  %y.2 = f32[] parameter(1)
  ROOT %add.2 = f32[] add(f32[] %x.2, f32[] %y.2)
}

ENTRY %main.356-layer_1_backward_spmd (param: f32[64], param.2: f32[128,64], param.4: f32[128], param.6: f32[64,128], param.1: f32[8,64], param.3: f32[8,128], param.5: f32[128,64], param.7: f32[8,64], param.8: f32[64,128]) -> (f32[64], f32[128,64], f32[128], f32[64,128], f32[8,64]) {
  %param = f32[64]{0} parameter(0), sharding={devices=[2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_1_backward$start"}
  %param.1 = f32[8,64]{1,0} parameter(4), sharding={devices=[2,2]0,1,2,3}, metadata={op_name="layer_1_backward$start"}
  %constant = f32[] constant(0.00048828125)
  %broadcast.1 = f32[8,64]{1,0} broadcast(f32[] %constant), dimensions={}
  %multiply.0 = f32[8,64]{1,0} multiply(f32[8,64]{1,0} %param.1, f32[8,64]{1,0} %broadcast.1), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/mul" source_file="/code/alpa/alpa/testing.py" source_line=103}
  %constant.3 = f32[] constant(0)
  %reduce = f32[64]{0} reduce(f32[8,64]{1,0} %multiply.0, f32[] %constant.3), dimensions={0}, to_apply=%region_1.161.layer_1_backward, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.3 = f32[64]{0} add(f32[64]{0} %param, f32[64]{0} %reduce), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/add"}
  %all-reduce = f32[64]{0} all-reduce(f32[64]{0} %add.3), channel_id=1, replica_groups={{0,2},{1,3}}, use_global_device_ids=true, to_apply=%region_1.161.layer_1_backward, sharding={devices=[2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.2 = f32[128,64]{1,0} parameter(1), sharding={devices=[1,2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_1_backward$start"}
  %param.3 = f32[8,128]{1,0} parameter(5), sharding={devices=[2,1,2]0,1,2,3 last_tile_dim_replicate}, metadata={op_name="layer_1_backward$start"}
  %dot.2 = f32[128,64]{1,0} dot(f32[8,128]{1,0} %param.3, f32[8,64]{1,0} %multiply.0), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/transpose(jvp(MLPModel))/Dense_3/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.4 = f32[128,64]{1,0} add(f32[128,64]{1,0} %param.2, f32[128,64]{1,0} %dot.2), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/add"}
  %all-reduce.1 = f32[128,64]{1,0} all-reduce(f32[128,64]{1,0} %add.4), channel_id=2, replica_groups={{0,2},{1,3}}, use_global_device_ids=true, to_apply=%add, sharding={devices=[1,2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.4 = f32[128]{0} parameter(2), sharding={replicated}, metadata={op_name="layer_1_backward$start"}
  %param.5 = f32[128,64]{1,0} parameter(6), sharding={devices=[1,2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_1_backward$start"}
  %dot.3 = f32[8,128]{1,0} dot(f32[8,64]{1,0} %multiply.0, f32[128,64]{1,0} %param.5), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/transpose(jvp(MLPModel))/Dense_3/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.2 = f32[8,128]{1,0} all-reduce(f32[8,128]{1,0} %dot.3), channel_id=3, replica_groups={{0,1},{2,3}}, use_global_device_ids=true, to_apply=%add.1, sharding={devices=[2,1,2]0,1,2,3 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/transpose(jvp(MLPModel))/Dense_3/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reduce.1 = f32[128]{0} reduce(f32[8,128]{1,0} %all-reduce.2, f32[] %constant.3), dimensions={0}, to_apply=%region_2.169.layer_1_backward, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.5 = f32[128]{0} add(f32[128]{0} %param.4, f32[128]{0} %reduce.1), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/add"}
  %all-reduce.3 = f32[128]{0} all-reduce(f32[128]{0} %add.5), channel_id=4, replica_groups={{0,2},{1,3}}, use_global_device_ids=true, to_apply=%region_2.169.layer_1_backward, sharding={replicated}, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.6 = f32[64,128]{1,0} parameter(3), sharding={devices=[2,1,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_1_backward$start"}
  %param.7 = f32[8,64]{1,0} parameter(7), sharding={devices=[2,2]0,1,2,3}, metadata={op_name="layer_1_backward$start"}
  %dot.4 = f32[64,128]{1,0} dot(f32[8,64]{1,0} %param.7, f32[8,128]{1,0} %all-reduce.2), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/transpose(jvp(MLPModel))/Dense_2/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.6 = f32[64,128]{1,0} add(f32[64,128]{1,0} %param.6, f32[64,128]{1,0} %dot.4), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/add"}
  %all-reduce.4 = f32[64,128]{1,0} all-reduce(f32[64,128]{1,0} %add.6), channel_id=5, replica_groups={{0,2},{1,3}}, use_global_device_ids=true, to_apply=%add.2, sharding={devices=[2,1,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.8 = f32[64,128]{1,0} parameter(8), sharding={devices=[2,1,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_1_backward$start"}
  %dot.5 = f32[8,64]{1,0} dot(f32[8,128]{1,0} %all-reduce.2, f32[64,128]{1,0} %param.8), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/transpose(jvp(MLPModel))/Dense_2/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %tuple = (f32[64]{0}, f32[128,64]{1,0}, f32[128]{0}, f32[64,128]{1,0}, f32[8,64]{1,0}) tuple(f32[64]{0} %all-reduce, f32[128,64]{1,0} %all-reduce.1, f32[128]{0} %all-reduce.3, f32[64,128]{1,0} %all-reduce.4, f32[8,64]{1,0} %dot.5), metadata={op_name="tuple.6"}
}

