HloModule train_step_pipeshard_parallel_mesh_0.47-layer_0, entry_computation_layout={(f32[128,64]{1,0},f32[64]{0},f32[64,128]{1,0},f32[128]{0},f32[8,128]{1,0})->(f32[8,128]{1,0}, f32[8,64]{1,0})}

%add (x: f32[], y: f32[]) -> f32[] {
  %x = f32[] parameter(0)
  %y = f32[] parameter(1)
  ROOT %add = f32[] add(f32[] %x, f32[] %y)
}

ENTRY %main.318-layer_0_spmd (param.1: f32[128,64], param.2: f32[64], param.3: f32[64,128], param.4: f32[128], param: f32[8,128]) -> (f32[8,128], f32[8,64]) {
  %param = f32[8,128]{1,0} parameter(4), sharding={devices=[2,1,2]0,1,2,3 last_tile_dim_replicate}, metadata={op_name="layer_0$start"}
  %param.1 = f32[128,64]{1,0} parameter(0), sharding={devices=[1,2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_0$start"}
  %dot = f32[8,64]{1,0} dot(f32[8,128]{1,0} %param, f32[128,64]{1,0} %param.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/jvp(MLPModel)/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.2 = f32[64]{0} parameter(1), sharding={devices=[2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_0$start"}
  %broadcast.2 = f32[8,64]{1,0} broadcast(f32[64]{0} %param.2), dimensions={1}
  %add.1 = f32[8,64]{1,0} add(f32[8,64]{1,0} %dot, f32[8,64]{1,0} %broadcast.2), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/jvp(MLPModel)/Dense_0/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.3 = f32[64,128]{1,0} parameter(2), sharding={devices=[2,1,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_0$start"}
  %dot.1 = f32[8,128]{1,0} dot(f32[8,64]{1,0} %add.1, f32[64,128]{1,0} %param.3), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/jvp(MLPModel)/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce = f32[8,128]{1,0} all-reduce(f32[8,128]{1,0} %dot.1), channel_id=1, replica_groups={{0,1},{2,3}}, use_global_device_ids=true, to_apply=%add, sharding={devices=[2,1,2]0,1,2,3 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/jvp(MLPModel)/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.4 = f32[128]{0} parameter(3), sharding={replicated}, metadata={op_name="layer_0$start"}
  %broadcast.3 = f32[8,128]{1,0} broadcast(f32[128]{0} %param.4), dimensions={1}
  %add.2 = f32[8,128]{1,0} add(f32[8,128]{1,0} %all-reduce, f32[8,128]{1,0} %broadcast.3), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/jvp(MLPModel)/Dense_1/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  ROOT %tuple = (f32[8,128]{1,0}, f32[8,64]{1,0}) tuple(f32[8,128]{1,0} %add.2, f32[8,64]{1,0} %add.1), metadata={op_name="tuple.3"}
}

