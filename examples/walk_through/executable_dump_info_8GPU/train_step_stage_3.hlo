HloModule train_step_pipeshard_parallel_mesh_0.47-layer_0_backward, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias) }, entry_computation_layout={(f32[128]{0},f32[64,128]{1,0},f32[64]{0},f32[128,64]{1,0},f32[16,128]{1,0},f32[8,64]{1,0},f32[8,128]{1,0},f32[64,128]{1,0})->(f32[128]{0}, f32[64,128]{1,0}, f32[64]{0}, f32[128,64]{1,0})}

%region_1.136.layer_0_backward (Arg_0.137: f32[], Arg_1.138: f32[]) -> f32[] {
  %Arg_0.137 = f32[] parameter(0)
  %Arg_1.138 = f32[] parameter(1)
  ROOT %add.139 = f32[] add(f32[] %Arg_0.137, f32[] %Arg_1.138), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation (param_1: f32[64], param_1.1: f32[8,64]) -> f32[64] {
  %param_1 = f32[64]{0} parameter(0)
  %param_1.1 = f32[8,64]{1,0} parameter(1)
  %constant_11 = f32[] constant(0)
  %reduce.2 = f32[64]{0} reduce(f32[8,64]{1,0} %param_1.1, f32[] %constant_11), dimensions={0}, to_apply=%region_1.136.layer_0_backward, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  ROOT %add.6 = f32[64]{0} add(f32[64]{0} %param_1, f32[64]{0} %reduce.2), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/add"}
}

%fused_computation.1 (param_0.1: f32[16,128], param_1.7: u32[4], param_2.7: u32[]) -> f32[8,128] {
  %param_0.1 = f32[16,128]{1,0} parameter(0)
  %constant_10 = u32[] constant(0)
  %param_1.7 = u32[4]{0} parameter(1)
  %param_2.7 = u32[] parameter(2)
  %dynamic-slice.4 = u32[1]{0} dynamic-slice(u32[4]{0} %param_1.7, u32[] %param_2.7), dynamic_slice_sizes={1}
  %bitcast.1 = u32[] bitcast(u32[1]{0} %dynamic-slice.4)
  %constant_9 = u32[] constant(1)
  %clamp.1 = u32[] clamp(u32[] %constant_10, u32[] %bitcast.1, u32[] %constant_9)
  %convert.1 = s32[] convert(u32[] %clamp.1)
  %constant_8 = s32[] constant(8)
  %multiply.2 = s32[] multiply(s32[] %convert.1, s32[] %constant_8)
  %constant_7 = s32[] constant(0)
  ROOT %dynamic-slice.3 = f32[8,128]{1,0} dynamic-slice(f32[16,128]{1,0} %param_0.1, s32[] %multiply.2, s32[] %constant_7), dynamic_slice_sizes={8,128}
}

%region_0.128.layer_0_backward (Arg_0.129: f32[], Arg_1.130: f32[]) -> f32[] {
  %Arg_0.129 = f32[] parameter(0)
  %Arg_1.130 = f32[] parameter(1)
  ROOT %add.131 = f32[] add(f32[] %Arg_0.129, f32[] %Arg_1.130), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.2 (param_0.2: f32[128], param_1.9: f32[16,128]) -> f32[128] {
  %param_0.2 = f32[128]{0} parameter(0)
  %param_1.9 = f32[16,128]{1,0} parameter(1)
  %constant_12 = f32[] constant(0)
  %reduce.3 = f32[128]{0} reduce(f32[16,128]{1,0} %param_1.9, f32[] %constant_12), dimensions={0}, to_apply=%region_0.128.layer_0_backward, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  ROOT %add.7 = f32[128]{0} add(f32[128]{0} %param_0.2, f32[128]{0} %reduce.3), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/add"}
}

%add (x: f32[], y: f32[]) -> f32[] {
  %x = f32[] parameter(0)
  %y = f32[] parameter(1)
  ROOT %add = f32[] add(f32[] %x, f32[] %y)
}

ENTRY %main.318-layer_0_backward_spmd (param: f32[128], param.2: f32[64,128], param.4: f32[64], param.6: f32[128,64], param.1: f32[16,128], param.3: f32[8,64], param.7: f32[8,128], param.5: f32[64,128]) -> (f32[128], f32[64,128], f32[64], f32[128,64]) {
  %param = f32[128]{0} parameter(0), sharding={replicated}, metadata={op_name="layer_0_backward$start"}
  %param.1 = f32[16,128]{1,0} parameter(4), sharding={replicated}, metadata={op_name="layer_0_backward$start"}
  %fusion.2 = f32[128]{0} fusion(f32[128]{0} %param, f32[16,128]{1,0} %param.1), kind=kLoop, calls=%fused_computation.2, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/add"}
  %param.3 = f32[8,64]{1,0} parameter(5), sharding={devices=[2,2]0,1,2,3}, metadata={op_name="layer_0_backward$start"}
  %constant_1 = u32[4]{0} constant({0, 0, 1, 1})
  %partition-id = u32[] partition-id()
  %fusion.1 = f32[8,128]{1,0} fusion(f32[16,128]{1,0} %param.1, u32[4]{0} %constant_1, u32[] %partition-id), kind=kLoop, calls=%fused_computation.1
  %param.2 = f32[64,128]{1,0} parameter(1), sharding={devices=[2,1,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_0_backward$start"}
  %cublas-gemm.3 = f32[64,128]{1,0} custom-call(f32[8,64]{1,0} %param.3, f32[8,128]{1,0} %fusion.1, f32[64,128]{1,0} %param.2), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/transpose(jvp(MLPModel))/Dense_1/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.2 = f32[8192]{0} bitcast(f32[64,128]{1,0} %cublas-gemm.3)
  %param.4 = f32[64]{0} parameter(2), sharding={devices=[2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_0_backward$start"}
  %param.5 = f32[64,128]{1,0} parameter(7), sharding={devices=[2,1,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_0_backward$start"}
  %cublas-gemm.5 = f32[8,64]{1,0} custom-call(f32[8,128]{1,0} %fusion.1, f32[64,128]{1,0} %param.5), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/transpose(jvp(MLPModel))/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion = f32[64]{0} fusion(f32[64]{0} %param.4, f32[8,64]{1,0} %cublas-gemm.5), kind=kLoop, calls=%fused_computation, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/add"}
  %param.7 = f32[8,128]{1,0} parameter(6), sharding={devices=[2,1,2]0,1,2,3 last_tile_dim_replicate}, metadata={op_name="layer_0_backward$start"}
  %param.6 = f32[128,64]{1,0} parameter(3), sharding={devices=[1,2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_0_backward$start"}
  %cublas-gemm.9 = f32[128,64]{1,0} custom-call(f32[8,128]{1,0} %param.7, f32[8,64]{1,0} %cublas-gemm.5, f32[128,64]{1,0} %param.6), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_0)/jit(main)/transpose(jvp(MLPModel))/Dense_0/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.4 = f32[8192]{0} bitcast(f32[128,64]{1,0} %cublas-gemm.9)
  %concatenate = f32[16448]{0} concatenate(f32[8192]{0} %bitcast.2, f32[64]{0} %fusion, f32[8192]{0} %bitcast.4), dimensions={0}
  %all-reduce.4 = f32[16448]{0} all-reduce(f32[16448]{0} %concatenate), channel_id=1, replica_groups={{0,2},{1,3}}, use_global_device_ids=true, to_apply=%add, metadata={op_name="grad_acc_skippable_all_reduce"}
  %slice = f32[8192]{0} slice(f32[16448]{0} %all-reduce.4), slice={[0:8192]}
  %bitcast.5 = f32[64,128]{1,0} bitcast(f32[8192]{0} %slice), sharding={devices=[2,1,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %slice.1 = f32[64]{0} slice(f32[16448]{0} %all-reduce.4), slice={[8192:8256]}, sharding={devices=[2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.2 = f32[8192]{0} slice(f32[16448]{0} %all-reduce.4), slice={[8256:16448]}
  %bitcast.7 = f32[128,64]{1,0} bitcast(f32[8192]{0} %slice.2), sharding={devices=[1,2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %tuple.2 = (f32[128]{0}, f32[64,128]{1,0}, f32[64]{0}, f32[128,64]{1,0}) tuple(f32[128]{0} %fusion.2, f32[64,128]{1,0} %bitcast.5, f32[64]{0} %slice.1, f32[128,64]{1,0} %bitcast.7)
}

