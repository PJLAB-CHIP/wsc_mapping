HloModule train_step_pipeshard_parallel_mesh_1.48-layer_1, input_output_alias={ {0}: (0, {}, may-alias) }, entry_computation_layout={(f32[],f32[8,64]{1,0},f32[64,128]{1,0},f32[128]{0},f32[128,64]{1,0},f32[64]{0},f32[8,64]{1,0})->(f32[], f32[8,64]{1,0}, f32[8,128]{1,0})}

%add (x: f32[], y: f32[]) -> f32[] {
  %x = f32[] parameter(0)
  %y = f32[] parameter(1)
  ROOT %add = f32[] add(f32[] %x, f32[] %y)
}

%region_0.91.layer_1 (Arg_0.92: f32[], Arg_1.93: f32[]) -> f32[] {
  %Arg_0.92 = f32[] parameter(0)
  %Arg_1.93 = f32[] parameter(1)
  ROOT %add.94 = f32[] add(f32[] %Arg_0.92, f32[] %Arg_1.93), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/alpa/testing.py" source_line=103}
}

%fused_computation (param_0.7: f32[8,64], param_1.8: f32[8,64]) -> f32[8,64] {
  %param_0.7 = f32[8,64]{1,0} parameter(0)
  %param_1.8 = f32[8,64]{1,0} parameter(1)
  %subtract.0 = f32[8,64]{1,0} subtract(f32[8,64]{1,0} %param_0.7, f32[8,64]{1,0} %param_1.8), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/sub" source_file="/code/alpa/alpa/testing.py" source_line=103}
  %constant_0 = f32[] constant(2)
  %broadcast.0 = f32[8,64]{1,0} broadcast(f32[] %constant_0), dimensions={}
  ROOT %multiply.0 = f32[8,64]{1,0} multiply(f32[8,64]{1,0} %subtract.0, f32[8,64]{1,0} %broadcast.0), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/mul" source_file="/code/alpa/alpa/testing.py" source_line=103}
}

%fused_computation.1 (param_0.1: f32[], param_1.3: f32[]) -> f32[] {
  %param_0.1 = f32[] parameter(0)
  %param_1.3 = f32[] parameter(1)
  %constant_1 = f32[] constant(0.00048828125)
  %multiply.4 = f32[] multiply(f32[] %param_1.3, f32[] %constant_1), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/div" source_file="/code/alpa/alpa/testing.py" source_line=103}
  ROOT %add.4 = f32[] add(f32[] %param_0.1, f32[] %multiply.4), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/add"}
}

%fused_computation.2 (param_0.8: f32[8,64], param_1.9: f32[8,64]) -> f32[] {
  %param_0.8 = f32[8,64]{1,0} parameter(0)
  %param_1.9 = f32[8,64]{1,0} parameter(1)
  %subtract.1 = f32[8,64]{1,0} subtract(f32[8,64]{1,0} %param_0.8, f32[8,64]{1,0} %param_1.9), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/sub" source_file="/code/alpa/alpa/testing.py" source_line=103}
  %multiply.5 = f32[8,64]{1,0} multiply(f32[8,64]{1,0} %subtract.1, f32[8,64]{1,0} %subtract.1), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/mul" source_file="/code/alpa/alpa/testing.py" source_line=103}
  %bitcast.1 = f32[512]{0} bitcast(f32[8,64]{1,0} %multiply.5)
  %constant_2 = f32[] constant(0)
  ROOT %reduce.2 = f32[] reduce(f32[512]{0} %bitcast.1, f32[] %constant_2), dimensions={0}, to_apply=%region_0.91.layer_1, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/alpa/testing.py" source_line=103}
}

%fused_computation.3 (param_0.6: f32[8,128], param_1.7: f32[128]) -> f32[8,128] {
  %param_0.6 = f32[8,128]{1,0} parameter(0)
  %param_1.7 = f32[128]{0} parameter(1)
  %broadcast.1 = f32[8,128]{1,0} broadcast(f32[128]{0} %param_1.7), dimensions={1}
  ROOT %add.5 = f32[8,128]{1,0} add(f32[8,128]{1,0} %param_0.6, f32[8,128]{1,0} %broadcast.1), metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/jvp(MLPModel)/Dense_2/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

ENTRY %main.356-layer_1_spmd (param: f32[], param.1: f32[8,64], param.2: f32[64,128], param.3: f32[128], param.4: f32[128,64], param.5: f32[64], param.6: f32[8,64]) -> (f32[], f32[8,64], f32[8,128]) {
  %param = f32[] parameter(0), sharding={replicated}, metadata={op_name="layer_1$start"}
  %param.1 = f32[8,64]{1,0} parameter(1), sharding={devices=[2,2]0,1,2,3}, metadata={op_name="layer_1$start"}
  %param.2 = f32[64,128]{1,0} parameter(2), sharding={devices=[2,1,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_1$start"}
  %cublas-gemm.1 = f32[8,128]{1,0} custom-call(f32[8,64]{1,0} %param.1, f32[64,128]{1,0} %param.2), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/jvp(MLPModel)/Dense_2/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce = f32[8,128]{1,0} all-reduce(f32[8,128]{1,0} %cublas-gemm.1), channel_id=1, replica_groups={{0,1},{2,3}}, use_global_device_ids=true, to_apply=%add, sharding={devices=[2,1,2]0,1,2,3 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/jvp(MLPModel)/Dense_2/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.3 = f32[128]{0} parameter(3), sharding={replicated}, metadata={op_name="layer_1$start"}
  %fusion.3 = f32[8,128]{1,0} fusion(f32[8,128]{1,0} %all-reduce, f32[128]{0} %param.3), kind=kLoop, calls=%fused_computation.3, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/jvp(MLPModel)/Dense_2/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.4 = f32[128,64]{1,0} parameter(4), sharding={devices=[1,2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_1$start"}
  %param.5 = f32[64]{0} parameter(5), sharding={devices=[2,2]0,2,1,3 last_tile_dim_replicate}, metadata={op_name="layer_1$start"}
  %broadcast.4 = f32[8,64]{1,0} broadcast(f32[64]{0} %param.5), dimensions={1}
  %cublas-gemm.5 = f32[8,64]{1,0} custom-call(f32[8,128]{1,0} %fusion.3, f32[128,64]{1,0} %param.4, f32[8,64]{1,0} %broadcast.4), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/jvp(MLPModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %param.6 = f32[8,64]{1,0} parameter(6), sharding={devices=[2,2]0,1,2,3}, metadata={op_name="layer_1$start"}
  %fusion.2 = f32[] fusion(f32[8,64]{1,0} %cublas-gemm.5, f32[8,64]{1,0} %param.6), kind=kInput, calls=%fused_computation.2, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/alpa/testing.py" source_line=103}
  %all-reduce.1 = f32[] all-reduce(f32[] %fusion.2), channel_id=2, replica_groups={{0,1},{2,3}}, use_global_device_ids=true, to_apply=%region_0.91.layer_1, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/alpa/testing.py" source_line=103}
  %fusion.1 = f32[] fusion(f32[] %param, f32[] %all-reduce.1), kind=kLoop, calls=%fused_computation.1, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/add"}
  %all-reduce.2 = f32[] all-reduce(f32[] %fusion.1), channel_id=3, replica_groups={{0,2},{1,3}}, use_global_device_ids=true, to_apply=%region_0.91.layer_1, sharding={replicated}, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/code/alpa/alpa/testing.py" source_line=103}
  %fusion = f32[8,64]{1,0} fusion(f32[8,64]{1,0} %cublas-gemm.5, f32[8,64]{1,0} %param.6), kind=kLoop, calls=%fused_computation, metadata={op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/jit(main)/mul" source_file="/code/alpa/alpa/testing.py" source_line=103}
  ROOT %tuple.1 = (f32[], f32[8,64]{1,0}, f32[8,128]{1,0}) tuple(f32[] %all-reduce.2, f32[8,64]{1,0} %fusion, f32[8,128]{1,0} %fusion.3)
}

